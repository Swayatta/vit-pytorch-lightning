{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/swayatta/miniconda3/envs/env1/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = torch.randint(1,100,(1,1,28,28))\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[29, 41,  2],\n",
       "          [ 3, 83, 15],\n",
       "          [44,  6, 86],\n",
       "          [85, 18, 64]],\n",
       "\n",
       "         [[31, 44, 56],\n",
       "          [12,  1, 73],\n",
       "          [30, 93, 33],\n",
       "          [65, 54, 97]],\n",
       "\n",
       "         [[33,  6, 95],\n",
       "          [56, 16, 22],\n",
       "          [32, 46, 51],\n",
       "          [44, 79, 99]],\n",
       "\n",
       "         [[13, 84, 32],\n",
       "          [47, 51, 19],\n",
       "          [ 7, 26, 37],\n",
       "          [44, 86, 98]]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = torch.randint(1,100,(1,4,4,3))\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange, reduce, repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3474692341.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    1. prenorm --> contains layernorm\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#### classes\n",
    "1. prenorm --> contains layernorm\n",
    "2. feedforward\n",
    "3. Attention --> qkv\n",
    "4. Transformers --> contains depth of multiple layers ?\n",
    "5. ViT --> Whole architecture, uses Transformers as a block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Running cells with 'env1' requires the notebook package.\n",
      "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "## Other implementation : https://medium.com/mlearning-ai/vision-transformers-from-scratch-pytorch-a-step-by-step-guide-96c3313c2e0c\n",
    "1. MyViT class\n",
    "    a) calls patchify\n",
    "    b) The patches need to go through a linear projection\n",
    "        nn. Linear is sufficient? Yes. Flattened input, linearly projected output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Running cells with 'env1' requires the notebook package.\n",
      "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "from data import patchify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchify(image, height_of_patch, width_of_patch, channel):\n",
    "    batch,seq_len, height_of_image, width_of_image, channel = image.shape\n",
    "    assert height_of_image % height_of_patch == 0 and width_of_image % width_of_patch == 0, 'Image dimensions must be divisible by patch dimensions'\n",
    "    \n",
    "    num_patches_height = height_of_image // height_of_patch\n",
    "    num_patches_width  = width_of_image // width_of_patch\n",
    "    \n",
    "    return rearrange(image, 'b n (h1 h) (w1 w) c-> b (n h1 w1) h w c', h1= num_patches_height, w1= num_patches_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 1, 4, 4, 3])\n",
      "torch.Size([40, 4, 2, 2, 3])\n",
      "Patches shape =  torch.Size([40, 4, 12])\n",
      "torch.Size([40, 4, 8])\n",
      "torch.Size([40, 1, 8])\n",
      "torch.Size([40, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# def patchify(image, height_of_patch, width_of_patch, channel):\n",
    "#     batch, height_of_image, width_of_image, channel = image.shape\n",
    "#     assert height_of_image % height_of_patch == 0 and width_of_image % width_of_patch == 0, 'Image dimensions must be divisible by patch dimensions'\n",
    "    \n",
    "#     num_patches_height = height_of_image // height_of_patch\n",
    "#     num_patches_width  = width_of_image // width_of_patch\n",
    "    \n",
    "    # return rearrange(image, 'b (h1 h) (w1 w) c-> (b h1 w1) h w c', h1= num_patches_height, w1= num_patches_width)\n",
    "\n",
    "LINEAR_EMBEDDING_DIM = 8\n",
    "\n",
    "sample = torch.randint(1,100,(40,1,4,4,3), dtype = torch.float) # B, H, W, C,   The sample needs to be in Float datatype\n",
    "print(sample.shape)\n",
    "patches = patchify(sample, 2,2,3)\n",
    "print(patches.shape)\n",
    "\n",
    "# so now, we have 40 patches lined up, each is a 2 by 2 image. \n",
    "# The paper says to flatten this\n",
    "patches = torch.flatten(patches, start_dim = 2, end_dim = -1)\n",
    "print(\"Patches shape = \",patches.shape) \n",
    "\n",
    "# linear mapping\n",
    "input_dim = patches.shape[-1]\n",
    "linear_embed_dim = LINEAR_EMBEDDING_DIM\n",
    "linear_mapper = nn.Linear(input_dim, linear_embed_dim)\n",
    "linear_maps = linear_mapper(patches)\n",
    "print(linear_maps.shape)\n",
    "\n",
    "# adding CLS token\n",
    "batch_size = linear_maps.shape[0]\n",
    "cls = nn.Parameter(torch.randn(batch_size,1,linear_embed_dim))\n",
    "print(cls.shape)\n",
    "\n",
    "linear_maps_with_cls = torch.cat([cls, linear_maps], dim = 1) # adding CLS token to the beginning of each linearly projected sample in the batch\n",
    "print(linear_maps_with_cls.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional encodings shape =  torch.Size([5, 8])\n"
     ]
    }
   ],
   "source": [
    "# Positional Encoding\n",
    "def getPositionEncoding(seq_len, d, n=10000):\n",
    "    P = np.zeros((seq_len, d))\n",
    "    for k in range(seq_len):\n",
    "        for i in np.arange(int(d/2)):\n",
    "            denominator = np.power(n, 2*i/d)\n",
    "            P[k, 2*i] = np.sin(k/denominator)\n",
    "            P[k, 2*i+1] = np.cos(k/denominator)\n",
    "    return torch.tensor(P, dtype = float)\n",
    "print( \"Positional encodings shape = \" ,getPositionEncoding(5,8).shape)\n",
    "positional_encoding = getPositionEncoding(5,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 5, 8])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings_to_transformer = (linear_maps_with_cls + positional_encoding).float()\n",
    "encodings_to_transformer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project(x):\n",
    "    num_attention_heads, attention_head_size = 12, 8\n",
    "    new_x_shape = x.size()[:-1] + (num_attention_heads, attention_head_size)\n",
    "    return x.view(*new_x_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cont vec shape =  torch.Size([40, 5, 96])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 5, 8])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def forward(x):\n",
    "    attention_head_size  = 8 # BERT_dim\n",
    "    num_attention_heads = 12\n",
    "    input_dim = 8\n",
    "    query = nn.Linear(8 , num_attention_heads * attention_head_size)\n",
    "    key = nn.Linear(8 , num_attention_heads * attention_head_size)\n",
    "    value = nn.Linear(8 , num_attention_heads * attention_head_size)\n",
    "    \n",
    "    \n",
    "    q = query(x)\n",
    "    k = key(x)\n",
    "    v = value(x)\n",
    "    \n",
    "    q,k,v = project(q), project(k), project(v) # B, N, num_of_attn_heads, head_dim\n",
    "    softmax = nn.Softmax(dim = -1)\n",
    "    attention_scores = softmax(torch.matmul(q, k.transpose(-1,-2)))\n",
    "    context_vector = torch.matmul(attention_scores, v)\n",
    "    context_vector = rearrange(context_vector, 'b n num_att h -> b n (num_att h)')\n",
    "    print(\"cont vec shape = \" , context_vector.shape)\n",
    "    \n",
    "    dense = nn.Linear(num_attention_heads * attention_head_size, input_dim)\n",
    "    output = dense(context_vector)\n",
    "    return output\n",
    "forward(encodings_to_transformer).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim, attention_head_size, num_attention_heads) -> None:\n",
    "        super().__init__()\n",
    "        total_head_size = num_attention_heads * attention_head_size\n",
    "        self.query = nn.Linear(dim , total_head_size )\n",
    "        self.key = nn.Linear(dim , total_head_size)\n",
    "        self.value = nn.Linear(dim , total_head_size)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "        self.dense = nn.Linear(total_head_size, dim)\n",
    "\n",
    "    def project(self, x):\n",
    "        num_attention_heads, attention_head_size = 12, 8\n",
    "        new_x_shape = x.size()[:-1] + (num_attention_heads, attention_head_size)\n",
    "        return x.view(*new_x_shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        \n",
    "        q,k,v = self.project(q), self.project(k), self.project(v) # B, N, num_of_attn_heads, head_dim\n",
    "        attention_scores = self.softmax(torch.matmul(q, k.transpose(-1,-2)))\n",
    "        context_vector = torch.matmul(attention_scores, v)\n",
    "        context_vector = rearrange(context_vector, 'b n num_att h -> b n (num_att h)')\n",
    "        output = self.dense(context_vector)\n",
    "        return output            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patchify():\n",
    "    def __init__(self, height_of_patch, width_of_patch) -> None:\n",
    "        self.h = height_of_patch\n",
    "        self.w = width_of_patch\n",
    "    def patchify(self, image):\n",
    "        batch, seq_len, height_of_image, width_of_image, channel = image.shape\n",
    "        assert height_of_image % self.h == 0 and width_of_image % self.w == 0, 'Image dimensions must be divisible by patch dimensions'\n",
    "        \n",
    "        num_patches_height = height_of_image // self.h\n",
    "        num_patches_width  = width_of_image // self.w\n",
    "        \n",
    "        return rearrange(image, 'b n (h1 h) (w1 w) c-> b (n h1 w1) h w c', h1= num_patches_height, w1= num_patches_width)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Projection_and_Add_CLS():\n",
    "    def __init__(self, batch_size, seq_len, height_of_patch, width_of_patch, channel, dim) -> None:\n",
    "        \n",
    "\n",
    "        total_patch_dim = height_of_patch * width_of_patch * channel\n",
    "        self.linear_mapper = nn.Linear(total_patch_dim, dim)\n",
    "        self.cls = nn.Parameter(torch.randn(batch_size,1,dim))\n",
    "\n",
    "    def project(self, patches):\n",
    "        patches = torch.flatten(patches, start_dim = 2, end_dim = -1)\n",
    "        linear_maps = self.linear_mapper(patches)\n",
    "        return linear_maps\n",
    "    \n",
    "    def add_cls(self, linear_maps):\n",
    "        # adding CLS token\n",
    "\n",
    "        linear_maps_with_cls = torch.cat([self.cls, linear_maps], dim = 1) # adding CLS token to the beginning of each linearly projected sample in the batch\n",
    "        return linear_maps_with_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding():\n",
    "    def __init__(self, seq_len, dim) -> None:\n",
    "        self.seq_len = seq_len\n",
    "        self.d = dim\n",
    "        self.n = 10000\n",
    "    def getPositionEncoding(self):\n",
    "        P = np.zeros((self.seq_len, self.d))\n",
    "        for k in range(self.seq_len):\n",
    "            for i in np.arange(int(self.d/2)):\n",
    "                denominator = np.power(self.n, 2*i/self.d)\n",
    "                P[k, 2*i] = np.sin(k/denominator)\n",
    "                P[k, 2*i+1] = np.cos(k/denominator)\n",
    "        return torch.tensor(P, dtype = float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transfomer(nn.Module):\n",
    "    def __init__(self, num_blocks, dim, mlp_dim, attention_head_size, num_attention_heads) -> None:\n",
    "        super().__init__()\n",
    "        # assert input_dim == attention_head_size\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.layers = nn.ModuleList([MultiHeadAttention(dim, attention_head_size, num_attention_heads) for _ in range(num_blocks)])\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_dim,dim)\n",
    "            )\n",
    "\n",
    "    def forward(self,x):\n",
    "        for layer in self.layers:\n",
    "            x = x+layer(self.norm1(x))\n",
    "            x = x + self.mlp(self.norm2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, batch_size, seq_len, height_of_patch, width_of_patch, channel, num_blocks, dim, mlp_dim, attention_head_size, num_attention_heads, num_of_labels) -> None:\n",
    "        super().__init__()\n",
    "        self.patchify = Patchify(height_of_patch, width_of_patch) ### TBD\n",
    "        self.linear_and_cls = Linear_Projection_and_Add_CLS(batch_size, seq_len, height_of_patch, width_of_patch, channel, dim)\n",
    "        self.pos_enc = PositionalEncoding(seq_len, dim)\n",
    "        self.transformer = Transfomer(num_blocks, dim, mlp_dim, attention_head_size, num_attention_heads)\n",
    "        self.linear = nn.Linear(dim, num_of_labels)\n",
    "\n",
    "    \n",
    "    \n",
    "    def forward(self, image):\n",
    "        patches = self.patchify.patchify(image)\n",
    "        linear_projections = self.linear_and_cls.project(patches)\n",
    "        linear_projections_with_cls = self.linear_and_cls.add_cls(linear_projections)\n",
    "        positional_encoding = self.pos_enc.getPositionEncoding()\n",
    "        input_to_transformer = (linear_projections_with_cls + positional_encoding).float()\n",
    "        output_from_transformer = self.transformer(input_to_transformer)\n",
    "        #use cls here\n",
    "        print(output.shape)\n",
    "        output = self.linear(output_from_transformer)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_samp = torch.randn(40,1,4,4,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (160x4 and 12x16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[173], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m# it should take the number of labels, channel, seq_len implicitly\u001b[39;00m\n\u001b[1;32m     15\u001b[0m vit \u001b[39m=\u001b[39m ViT(batch_size, seq_len, height_of_patch, width_of_patch, channel, num_blocks, dim, mlp_dim, attention_head_size, num_attention_heads, num_of_labels)\n\u001b[0;32m---> 16\u001b[0m vit(new_samp)\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m~/miniconda3/envs/env1/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[172], line 14\u001b[0m, in \u001b[0;36mViT.forward\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, image):\n\u001b[1;32m     13\u001b[0m     patches \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatchify\u001b[39m.\u001b[39mpatchify(image)\n\u001b[0;32m---> 14\u001b[0m     linear_projections \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear_and_cls\u001b[39m.\u001b[39;49mproject(patches)\n\u001b[1;32m     15\u001b[0m     linear_projections_with_cls \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear_and_cls\u001b[39m.\u001b[39madd_cls(linear_projections)\n\u001b[1;32m     16\u001b[0m     positional_encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_enc\u001b[39m.\u001b[39mgetPositionEncoding()\n",
      "Cell \u001b[0;32mIn[122], line 11\u001b[0m, in \u001b[0;36mLinear_Projection_and_Add_CLS.project\u001b[0;34m(self, patches)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mproject\u001b[39m(\u001b[39mself\u001b[39m, patches):\n\u001b[1;32m     10\u001b[0m     patches \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(patches, start_dim \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m, end_dim \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m     linear_maps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear_mapper(patches)\n\u001b[1;32m     12\u001b[0m     \u001b[39mreturn\u001b[39;00m linear_maps\n",
      "File \u001b[0;32m~/miniconda3/envs/env1/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/env1/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (160x4 and 12x16)"
     ]
    }
   ],
   "source": [
    "batch_size = 40\n",
    "seq_len = 1\n",
    "height_of_patch = 2\n",
    "width_of_patch = 2\n",
    "channel = 3\n",
    "num_blocks = 4\n",
    "dim = 16\n",
    "mlp_dim = 9\n",
    "attention_head_size = 8\n",
    "num_attention_heads = 12\n",
    "num_of_labels = 2\n",
    "\n",
    "# it should take the number of labels, channel, seq_len implicitly\n",
    "\n",
    "vit = ViT(batch_size, seq_len, height_of_patch, width_of_patch, channel, num_blocks, dim, mlp_dim, attention_head_size, num_attention_heads, num_of_labels)\n",
    "vit(new_samp).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch import optim, nn, utils, Tensor\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor, Compose\n",
    "import lightning.pytorch as pl\n",
    "dataset = MNIST(os.getcwd(), download=True, transform=Compose([ToTensor()]))\n",
    "train_loader = utils.data.DataLoader(dataset, batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 28, 28])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit import ViT\n",
    "vit = ViT(batch_size, height_of_patch, width_of_patch, channel, num_blocks, dim, mlp_dim, attention_head_size, num_attention_heads, num_of_labels)\n",
    "new_samp = torch.randn(40,1,4,4,1)\n",
    "print(vit(new_samp).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([40, 5, 10])\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from vit import ViT\n",
    "import os\n",
    "import torch\n",
    "from torch import optim, nn, utils, Tensor\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "\n",
    "with open('config/config.yaml', 'rb') as f:\n",
    "    conf = yaml.safe_load(f.read())    # load the config file\n",
    "\n",
    "\n",
    "batch_size = conf['batch_size']\n",
    "height_of_patch = conf['height_of_patch']\n",
    "width_of_patch = conf['width_of_patch']\n",
    "channel = conf['channel']\n",
    "num_blocks = conf['num_blocks']\n",
    "dim = conf['dim']\n",
    "mlp_dim = conf['mlp_dim']\n",
    "attention_head_size = conf['attention_head_size']\n",
    "num_attention_heads = conf['num_attention_heads']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Defining lightning module\n",
    "\n",
    "class LitVit(pl.LightningModule):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.vit = vit\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        # it is independent of forward\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.vit(x)\n",
    "        loss = nn.functional.mse_loss(y, x)\n",
    "        # Logging to TensorBoard (if installed) by default\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "# Loading MNIST ###\n",
    "\n",
    "dataset = MNIST(os.getcwd(), download=True, transform=ToTensor())\n",
    "train_loader = utils.data.DataLoader(dataset)\n",
    "\n",
    "\n",
    "\n",
    "### Loading a subsample of mnist ###\n",
    "K = 6000 # enter your length here\n",
    "subsample_train_indices = torch.randperm(len(dataset))[:K]\n",
    "train_loader = utils.data.DataLoader(dataset, batch_size=batch_size, sampler=utils.data.SubsetRandomSampler(subsample_train_indices))\n",
    "\n",
    "\n",
    "\n",
    "num_of_labels = len(train_loader.dataset.classes)\n",
    "\n",
    "\n",
    "print(num_of_labels)\n",
    "\n",
    "\n",
    "vit = ViT(batch_size, height_of_patch, width_of_patch, channel, num_blocks, dim, mlp_dim, attention_head_size, num_attention_heads, num_of_labels)\n",
    "new_samp = torch.randn(40,1,4,4,1)\n",
    "print(vit(new_samp).shape)\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 197, 10])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "0D or 1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[171], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m z \u001b[39m=\u001b[39m vit(x)\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(z\u001b[39m.\u001b[39mshape)\n\u001b[0;32m----> 7\u001b[0m loss \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mcross_entropy(y\u001b[39m.\u001b[39;49mfloat(),z)\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(loss\u001b[39m.\u001b[39mitem)\n\u001b[1;32m      9\u001b[0m \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/env1/lib/python3.10/site-packages/torch/nn/functional.py:3014\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3012\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3013\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3014\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 0D or 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "for x,y in train_loader:\n",
    "    b,c,img_h,img_w = x.shape\n",
    "    x = x.view(b,1,img_h, img_w, channel)\n",
    "    z = vit(x)\n",
    "    print(z.shape)\n",
    "    \n",
    "    loss = nn.functional.cross_entropy(y.float(),z)\n",
    "    print(loss.item)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
